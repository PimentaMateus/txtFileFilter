2019-10-29 15:11:09 WARN  Utils:66 - Your hostname, PimentaUbuntu resolves to a loopback address: 127.0.1.1; using 192.168.15.12 instead (on interface wlp3s0)
2019-10-29 15:11:09 WARN  Utils:66 - Set SPARK_LOCAL_IP if you need to bind to another address
2019-10-29 15:11:09 WARN  NativeCodeLoader:62 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2019-10-29 15:11:09 INFO  SparkContext:54 - Running Spark version 2.3.2
2019-10-29 15:11:09 INFO  SparkContext:54 - Submitted application: streamDM
2019-10-29 15:11:09 INFO  SecurityManager:54 - Changing view acls to: mateus
2019-10-29 15:11:09 INFO  SecurityManager:54 - Changing modify acls to: mateus
2019-10-29 15:11:09 INFO  SecurityManager:54 - Changing view acls groups to: 
2019-10-29 15:11:09 INFO  SecurityManager:54 - Changing modify acls groups to: 
2019-10-29 15:11:09 INFO  SecurityManager:54 - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(mateus); groups with view permissions: Set(); users  with modify permissions: Set(mateus); groups with modify permissions: Set()
2019-10-29 15:11:10 INFO  Utils:54 - Successfully started service 'sparkDriver' on port 37941.
2019-10-29 15:11:10 INFO  SparkEnv:54 - Registering MapOutputTracker
2019-10-29 15:11:10 INFO  SparkEnv:54 - Registering BlockManagerMaster
2019-10-29 15:11:10 INFO  BlockManagerMasterEndpoint:54 - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2019-10-29 15:11:10 INFO  BlockManagerMasterEndpoint:54 - BlockManagerMasterEndpoint up
2019-10-29 15:11:10 INFO  DiskBlockManager:54 - Created local directory at /tmp/blockmgr-a9e8716f-6894-4fe7-85b3-93462b29efff
2019-10-29 15:11:10 INFO  MemoryStore:54 - MemoryStore started with capacity 366.3 MB
2019-10-29 15:11:10 INFO  SparkEnv:54 - Registering OutputCommitCoordinator
2019-10-29 15:11:10 INFO  log:192 - Logging initialized @1885ms
2019-10-29 15:11:10 INFO  Server:351 - jetty-9.3.z-SNAPSHOT, build timestamp: unknown, git hash: unknown
2019-10-29 15:11:10 INFO  Server:419 - Started @1997ms
2019-10-29 15:11:10 INFO  AbstractConnector:278 - Started ServerConnector@72efb5c1{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2019-10-29 15:11:10 INFO  Utils:54 - Successfully started service 'SparkUI' on port 4040.
2019-10-29 15:11:10 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@5bd1ceca{/jobs,null,AVAILABLE,@Spark}
2019-10-29 15:11:10 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@2ef8a8c3{/jobs/json,null,AVAILABLE,@Spark}
2019-10-29 15:11:10 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@24f43aa3{/jobs/job,null,AVAILABLE,@Spark}
2019-10-29 15:11:10 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@1e11bc55{/jobs/job/json,null,AVAILABLE,@Spark}
2019-10-29 15:11:10 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@7544a1e4{/stages,null,AVAILABLE,@Spark}
2019-10-29 15:11:10 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@70e0accd{/stages/json,null,AVAILABLE,@Spark}
2019-10-29 15:11:10 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@7957dc72{/stages/stage,null,AVAILABLE,@Spark}
2019-10-29 15:11:10 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@4fdfa676{/stages/stage/json,null,AVAILABLE,@Spark}
2019-10-29 15:11:10 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@82c57b3{/stages/pool,null,AVAILABLE,@Spark}
2019-10-29 15:11:10 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@5be82d43{/stages/pool/json,null,AVAILABLE,@Spark}
2019-10-29 15:11:10 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@600b0b7{/storage,null,AVAILABLE,@Spark}
2019-10-29 15:11:10 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@345e5a17{/storage/json,null,AVAILABLE,@Spark}
2019-10-29 15:11:10 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@5ea502e0{/storage/rdd,null,AVAILABLE,@Spark}
2019-10-29 15:11:10 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@443dbe42{/storage/rdd/json,null,AVAILABLE,@Spark}
2019-10-29 15:11:10 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@473b3b7a{/environment,null,AVAILABLE,@Spark}
2019-10-29 15:11:10 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@1734f68{/environment/json,null,AVAILABLE,@Spark}
2019-10-29 15:11:10 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@77b7ffa4{/executors,null,AVAILABLE,@Spark}
2019-10-29 15:11:10 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@5ed190be{/executors/json,null,AVAILABLE,@Spark}
2019-10-29 15:11:10 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@402f80f5{/executors/threadDump,null,AVAILABLE,@Spark}
2019-10-29 15:11:10 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@5bbc9f97{/executors/threadDump/json,null,AVAILABLE,@Spark}
2019-10-29 15:11:10 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@133e019b{/static,null,AVAILABLE,@Spark}
2019-10-29 15:11:10 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@213e3629{/,null,AVAILABLE,@Spark}
2019-10-29 15:11:10 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@4e9658b5{/api,null,AVAILABLE,@Spark}
2019-10-29 15:11:10 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@c41709a{/jobs/job/kill,null,AVAILABLE,@Spark}
2019-10-29 15:11:10 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@7db0565c{/stages/stage/kill,null,AVAILABLE,@Spark}
2019-10-29 15:11:10 INFO  SparkUI:54 - Bound SparkUI to 0.0.0.0, and started at http://192.168.15.12:4040
2019-10-29 15:11:10 INFO  SparkContext:54 - Added JAR file:/home/mateus/ProjetosSpark/streamDM-master/scripts/../target/scala-2.11/streamdm-spark-streaming-_2.11-0.2.jar at spark://192.168.15.12:37941/jars/streamdm-spark-streaming-_2.11-0.2.jar with timestamp 1572372670686
2019-10-29 15:11:10 INFO  Executor:54 - Starting executor ID driver on host localhost
2019-10-29 15:11:10 INFO  Utils:54 - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 40031.
2019-10-29 15:11:10 INFO  NettyBlockTransferService:54 - Server created on 192.168.15.12:40031
2019-10-29 15:11:10 INFO  BlockManager:54 - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2019-10-29 15:11:10 INFO  BlockManagerMaster:54 - Registering BlockManager BlockManagerId(driver, 192.168.15.12, 40031, None)
2019-10-29 15:11:10 INFO  BlockManagerMasterEndpoint:54 - Registering block manager 192.168.15.12:40031 with 366.3 MB RAM, BlockManagerId(driver, 192.168.15.12, 40031, None)
2019-10-29 15:11:10 INFO  BlockManagerMaster:54 - Registered BlockManager BlockManagerId(driver, 192.168.15.12, 40031, None)
2019-10-29 15:11:10 INFO  BlockManager:54 - Initialized BlockManager: BlockManagerId(driver, 192.168.15.12, 40031, None)
2019-10-29 15:11:11 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@55ea2d70{/metrics/json,null,AVAILABLE,@Spark}
2019-10-29 15:11:11 INFO  FileReader:54 - interested
2019-10-29 15:11:11 INFO  FileReader:54 - notInterested
2019-10-29 15:11:11 INFO  LearningNodeNBAdaptive:54 - 0 1 3 4 2
2019-10-29 15:11:11 INFO  FileReader$$anon$1:54 - Slide time = 100 ms
2019-10-29 15:11:11 INFO  FileReader$$anon$1:54 - Storage level = Serialized 1x Replicated
2019-10-29 15:11:11 INFO  FileReader$$anon$1:54 - Checkpoint interval = null
2019-10-29 15:11:11 INFO  FileReader$$anon$1:54 - Remember interval = 100 ms
2019-10-29 15:11:11 INFO  FileReader$$anon$1:54 - Initialized and validated org.apache.spark.streamdm.streams.FileReader$$anon$1@36112a68
2019-10-29 15:11:11 INFO  ForEachDStream:54 - Slide time = 100 ms
2019-10-29 15:11:11 INFO  ForEachDStream:54 - Storage level = Serialized 1x Replicated
2019-10-29 15:11:11 INFO  ForEachDStream:54 - Checkpoint interval = null
2019-10-29 15:11:11 INFO  ForEachDStream:54 - Remember interval = 100 ms
2019-10-29 15:11:11 INFO  ForEachDStream:54 - Initialized and validated org.apache.spark.streaming.dstream.ForEachDStream@124db5be
2019-10-29 15:11:11 INFO  FileReader$$anon$1:54 - Slide time = 100 ms
2019-10-29 15:11:11 INFO  FileReader$$anon$1:54 - Storage level = Serialized 1x Replicated
2019-10-29 15:11:11 INFO  FileReader$$anon$1:54 - Checkpoint interval = null
2019-10-29 15:11:11 INFO  FileReader$$anon$1:54 - Remember interval = 100 ms
2019-10-29 15:11:11 INFO  FileReader$$anon$1:54 - Initialized and validated org.apache.spark.streamdm.streams.FileReader$$anon$1@36112a68
2019-10-29 15:11:11 INFO  MappedDStream:54 - Slide time = 100 ms
2019-10-29 15:11:11 INFO  MappedDStream:54 - Storage level = Serialized 1x Replicated
2019-10-29 15:11:11 INFO  MappedDStream:54 - Checkpoint interval = null
2019-10-29 15:11:11 INFO  MappedDStream:54 - Remember interval = 100 ms
2019-10-29 15:11:11 INFO  MappedDStream:54 - Initialized and validated org.apache.spark.streaming.dstream.MappedDStream@1484d744
2019-10-29 15:11:11 INFO  MappedDStream:54 - Slide time = 100 ms
2019-10-29 15:11:11 INFO  MappedDStream:54 - Storage level = Serialized 1x Replicated
2019-10-29 15:11:11 INFO  MappedDStream:54 - Checkpoint interval = null
2019-10-29 15:11:11 INFO  MappedDStream:54 - Remember interval = 100 ms
2019-10-29 15:11:11 INFO  MappedDStream:54 - Initialized and validated org.apache.spark.streaming.dstream.MappedDStream@4992eade
2019-10-29 15:11:11 INFO  MappedDStream:54 - Slide time = 100 ms
2019-10-29 15:11:11 INFO  MappedDStream:54 - Storage level = Serialized 1x Replicated
2019-10-29 15:11:11 INFO  MappedDStream:54 - Checkpoint interval = null
2019-10-29 15:11:11 INFO  MappedDStream:54 - Remember interval = 100 ms
2019-10-29 15:11:11 INFO  MappedDStream:54 - Initialized and validated org.apache.spark.streaming.dstream.MappedDStream@766fcbee
2019-10-29 15:11:11 INFO  ShuffledDStream:54 - Slide time = 100 ms
2019-10-29 15:11:11 INFO  ShuffledDStream:54 - Storage level = Serialized 1x Replicated
2019-10-29 15:11:11 INFO  ShuffledDStream:54 - Checkpoint interval = null
2019-10-29 15:11:11 INFO  ShuffledDStream:54 - Remember interval = 100 ms
2019-10-29 15:11:11 INFO  ShuffledDStream:54 - Initialized and validated org.apache.spark.streaming.dstream.ShuffledDStream@9f070d8
2019-10-29 15:11:11 INFO  MappedDStream:54 - Slide time = 100 ms
2019-10-29 15:11:11 INFO  MappedDStream:54 - Storage level = Serialized 1x Replicated
2019-10-29 15:11:11 INFO  MappedDStream:54 - Checkpoint interval = null
2019-10-29 15:11:11 INFO  MappedDStream:54 - Remember interval = 100 ms
2019-10-29 15:11:11 INFO  MappedDStream:54 - Initialized and validated org.apache.spark.streaming.dstream.MappedDStream@3bfca855
2019-10-29 15:11:11 INFO  MappedDStream:54 - Slide time = 100 ms
2019-10-29 15:11:11 INFO  MappedDStream:54 - Storage level = Serialized 1x Replicated
2019-10-29 15:11:11 INFO  MappedDStream:54 - Checkpoint interval = null
2019-10-29 15:11:11 INFO  MappedDStream:54 - Remember interval = 100 ms
2019-10-29 15:11:11 INFO  MappedDStream:54 - Initialized and validated org.apache.spark.streaming.dstream.MappedDStream@65e661ee
2019-10-29 15:11:11 INFO  ForEachDStream:54 - Slide time = 100 ms
2019-10-29 15:11:11 INFO  ForEachDStream:54 - Storage level = Serialized 1x Replicated
2019-10-29 15:11:11 INFO  ForEachDStream:54 - Checkpoint interval = null
2019-10-29 15:11:11 INFO  ForEachDStream:54 - Remember interval = 100 ms
2019-10-29 15:11:11 INFO  ForEachDStream:54 - Initialized and validated org.apache.spark.streaming.dstream.ForEachDStream@6d5ee607
2019-10-29 15:11:11 INFO  FileReader$$anon$1:54 - File reading gets started.
2019-10-29 15:11:11 INFO  RecurringTimer:54 - Started timer for JobGenerator at time 1572372672000
2019-10-29 15:11:11 INFO  JobGenerator:54 - Started JobGenerator at 1572372672000 ms
2019-10-29 15:11:11 INFO  JobScheduler:54 - Started JobScheduler
2019-10-29 15:11:11 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@37d871c2{/streaming,null,AVAILABLE,@Spark}
2019-10-29 15:11:11 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@3baf6936{/streaming/json,null,AVAILABLE,@Spark}
2019-10-29 15:11:11 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@7a904f32{/streaming/batch,null,AVAILABLE,@Spark}
2019-10-29 15:11:11 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@2b59501e{/streaming/batch/json,null,AVAILABLE,@Spark}
2019-10-29 15:11:11 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@36fc05ff{/static/streaming,null,AVAILABLE,@Spark}
2019-10-29 15:11:11 INFO  StreamingContext:54 - StreamingContext started
2019-10-29 15:11:12 INFO  JobScheduler:54 - Added jobs for time 1572372672000 ms
2019-10-29 15:11:12 INFO  JobScheduler:54 - Starting job streaming job 1572372672000 ms.0 from job set of time 1572372672000 ms
2019-10-29 15:11:12 INFO  SparkContext:54 - Starting job: aggregate at HoeffdingTree.scala:149
2019-10-29 15:11:12 INFO  DAGScheduler:54 - Got job 0 (aggregate at HoeffdingTree.scala:149) with 2 output partitions
2019-10-29 15:11:12 INFO  DAGScheduler:54 - Final stage: ResultStage 0 (aggregate at HoeffdingTree.scala:149)
2019-10-29 15:11:12 INFO  DAGScheduler:54 - Parents of final stage: List()
2019-10-29 15:11:12 INFO  DAGScheduler:54 - Missing parents: List()
2019-10-29 15:11:12 INFO  DAGScheduler:54 - Submitting ResultStage 0 (ParallelCollectionRDD[0] at main at NativeMethodAccessorImpl.java:0), which has no missing parents
2019-10-29 15:11:12 INFO  MemoryStore:54 - Block broadcast_0 stored as values in memory (estimated size 7.8 KB, free 366.3 MB)
2019-10-29 15:11:12 INFO  MemoryStore:54 - Block broadcast_0_piece0 stored as bytes in memory (estimated size 3.5 KB, free 366.3 MB)
2019-10-29 15:11:12 INFO  BlockManagerInfo:54 - Added broadcast_0_piece0 in memory on 192.168.15.12:40031 (size: 3.5 KB, free: 366.3 MB)
2019-10-29 15:11:12 INFO  SparkContext:54 - Created broadcast 0 from broadcast at DAGScheduler.scala:1039
2019-10-29 15:11:12 INFO  DAGScheduler:54 - Submitting 2 missing tasks from ResultStage 0 (ParallelCollectionRDD[0] at main at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))
2019-10-29 15:11:12 INFO  TaskSchedulerImpl:54 - Adding task set 0.0 with 2 tasks
2019-10-29 15:11:12 INFO  TaskSetManager:54 - Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 13295 bytes)
2019-10-29 15:11:12 INFO  TaskSetManager:54 - Starting task 1.0 in stage 0.0 (TID 1, localhost, executor driver, partition 1, PROCESS_LOCAL, 13295 bytes)
2019-10-29 15:11:12 INFO  Executor:54 - Running task 1.0 in stage 0.0 (TID 1)
2019-10-29 15:11:12 INFO  Executor:54 - Running task 0.0 in stage 0.0 (TID 0)
2019-10-29 15:11:12 INFO  Executor:54 - Fetching spark://192.168.15.12:37941/jars/streamdm-spark-streaming-_2.11-0.2.jar with timestamp 1572372670686
2019-10-29 15:11:12 INFO  TransportClientFactory:267 - Successfully created connection to /192.168.15.12:37941 after 36 ms (0 ms spent in bootstraps)
2019-10-29 15:11:12 INFO  Utils:54 - Fetching spark://192.168.15.12:37941/jars/streamdm-spark-streaming-_2.11-0.2.jar to /tmp/spark-0b7e3a47-b4c1-453e-adac-a3b50b7716f1/userFiles-6a04e24c-02cb-453b-976c-fa44f5799baa/fetchFileTemp5292605732514855031.tmp
2019-10-29 15:11:12 INFO  Executor:54 - Adding file:/tmp/spark-0b7e3a47-b4c1-453e-adac-a3b50b7716f1/userFiles-6a04e24c-02cb-453b-976c-fa44f5799baa/streamdm-spark-streaming-_2.11-0.2.jar to class loader
2019-10-29 15:11:12 ERROR Executor:91 - Exception in task 1.0 in stage 0.0 (TID 1)
java.lang.ArrayIndexOutOfBoundsException: 6
	at org.apache.spark.streamdm.classifiers.trees.NominalFeatureClassObserver.observeClass(FeatureClassObserver.scala:155)
	at org.apache.spark.streamdm.classifiers.trees.nodes.ActiveLearningNode$$anonfun$learn$1.apply(ActiveLearningNode.scala:73)
	at org.apache.spark.streamdm.classifiers.trees.nodes.ActiveLearningNode$$anonfun$learn$1.apply(ActiveLearningNode.scala:73)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at org.apache.spark.streamdm.classifiers.trees.nodes.ActiveLearningNode.learn(ActiveLearningNode.scala:72)
	at org.apache.spark.streamdm.classifiers.trees.nodes.LearningNodeNBAdaptive.learn(LearningNodeNBAdaptive.scala:68)
	at org.apache.spark.streamdm.classifiers.trees.HoeffdingTreeModel.update(HoeffdingTree.scala:230)
	at org.apache.spark.streamdm.classifiers.trees.HoeffdingTree$$anonfun$train$1$$anonfun$1.apply(HoeffdingTree.scala:150)
	at org.apache.spark.streamdm.classifiers.trees.HoeffdingTree$$anonfun$train$1$$anonfun$1.apply(HoeffdingTree.scala:150)
	at scala.collection.TraversableOnce$$anonfun$foldLeft$1.apply(TraversableOnce.scala:157)
	at scala.collection.TraversableOnce$$anonfun$foldLeft$1.apply(TraversableOnce.scala:157)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)
	at scala.collection.TraversableOnce$class.foldLeft(TraversableOnce.scala:157)
	at org.apache.spark.InterruptibleIterator.foldLeft(InterruptibleIterator.scala:28)
	at scala.collection.TraversableOnce$class.aggregate(TraversableOnce.scala:214)
	at org.apache.spark.InterruptibleIterator.aggregate(InterruptibleIterator.scala:28)
	at org.apache.spark.rdd.RDD$$anonfun$aggregate$1$$anonfun$21.apply(RDD.scala:1122)
	at org.apache.spark.rdd.RDD$$anonfun$aggregate$1$$anonfun$21.apply(RDD.scala:1122)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:2130)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:2130)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:109)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2019-10-29 15:11:12 ERROR Executor:91 - Exception in task 0.0 in stage 0.0 (TID 0)
java.lang.ArrayIndexOutOfBoundsException: 7
	at org.apache.spark.streamdm.classifiers.trees.NominalFeatureClassObserver.observeClass(FeatureClassObserver.scala:155)
	at org.apache.spark.streamdm.classifiers.trees.nodes.ActiveLearningNode$$anonfun$learn$1.apply(ActiveLearningNode.scala:73)
	at org.apache.spark.streamdm.classifiers.trees.nodes.ActiveLearningNode$$anonfun$learn$1.apply(ActiveLearningNode.scala:73)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at org.apache.spark.streamdm.classifiers.trees.nodes.ActiveLearningNode.learn(ActiveLearningNode.scala:72)
	at org.apache.spark.streamdm.classifiers.trees.nodes.LearningNodeNBAdaptive.learn(LearningNodeNBAdaptive.scala:68)
	at org.apache.spark.streamdm.classifiers.trees.HoeffdingTreeModel.update(HoeffdingTree.scala:230)
	at org.apache.spark.streamdm.classifiers.trees.HoeffdingTree$$anonfun$train$1$$anonfun$1.apply(HoeffdingTree.scala:150)
	at org.apache.spark.streamdm.classifiers.trees.HoeffdingTree$$anonfun$train$1$$anonfun$1.apply(HoeffdingTree.scala:150)
	at scala.collection.TraversableOnce$$anonfun$foldLeft$1.apply(TraversableOnce.scala:157)
	at scala.collection.TraversableOnce$$anonfun$foldLeft$1.apply(TraversableOnce.scala:157)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)
	at scala.collection.TraversableOnce$class.foldLeft(TraversableOnce.scala:157)
	at org.apache.spark.InterruptibleIterator.foldLeft(InterruptibleIterator.scala:28)
	at scala.collection.TraversableOnce$class.aggregate(TraversableOnce.scala:214)
	at org.apache.spark.InterruptibleIterator.aggregate(InterruptibleIterator.scala:28)
	at org.apache.spark.rdd.RDD$$anonfun$aggregate$1$$anonfun$21.apply(RDD.scala:1122)
	at org.apache.spark.rdd.RDD$$anonfun$aggregate$1$$anonfun$21.apply(RDD.scala:1122)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:2130)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:2130)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:109)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2019-10-29 15:11:12 WARN  TaskSetManager:66 - Lost task 1.0 in stage 0.0 (TID 1, localhost, executor driver): java.lang.ArrayIndexOutOfBoundsException: 6
	at org.apache.spark.streamdm.classifiers.trees.NominalFeatureClassObserver.observeClass(FeatureClassObserver.scala:155)
	at org.apache.spark.streamdm.classifiers.trees.nodes.ActiveLearningNode$$anonfun$learn$1.apply(ActiveLearningNode.scala:73)
	at org.apache.spark.streamdm.classifiers.trees.nodes.ActiveLearningNode$$anonfun$learn$1.apply(ActiveLearningNode.scala:73)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at org.apache.spark.streamdm.classifiers.trees.nodes.ActiveLearningNode.learn(ActiveLearningNode.scala:72)
	at org.apache.spark.streamdm.classifiers.trees.nodes.LearningNodeNBAdaptive.learn(LearningNodeNBAdaptive.scala:68)
	at org.apache.spark.streamdm.classifiers.trees.HoeffdingTreeModel.update(HoeffdingTree.scala:230)
	at org.apache.spark.streamdm.classifiers.trees.HoeffdingTree$$anonfun$train$1$$anonfun$1.apply(HoeffdingTree.scala:150)
	at org.apache.spark.streamdm.classifiers.trees.HoeffdingTree$$anonfun$train$1$$anonfun$1.apply(HoeffdingTree.scala:150)
	at scala.collection.TraversableOnce$$anonfun$foldLeft$1.apply(TraversableOnce.scala:157)
	at scala.collection.TraversableOnce$$anonfun$foldLeft$1.apply(TraversableOnce.scala:157)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)
	at scala.collection.TraversableOnce$class.foldLeft(TraversableOnce.scala:157)
	at org.apache.spark.InterruptibleIterator.foldLeft(InterruptibleIterator.scala:28)
	at scala.collection.TraversableOnce$class.aggregate(TraversableOnce.scala:214)
	at org.apache.spark.InterruptibleIterator.aggregate(InterruptibleIterator.scala:28)
	at org.apache.spark.rdd.RDD$$anonfun$aggregate$1$$anonfun$21.apply(RDD.scala:1122)
	at org.apache.spark.rdd.RDD$$anonfun$aggregate$1$$anonfun$21.apply(RDD.scala:1122)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:2130)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:2130)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:109)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

2019-10-29 15:11:12 ERROR TaskSetManager:70 - Task 1 in stage 0.0 failed 1 times; aborting job
2019-10-29 15:11:12 INFO  TaskSchedulerImpl:54 - Removed TaskSet 0.0, whose tasks have all completed, from pool 
2019-10-29 15:11:12 WARN  TaskSetManager:66 - Lost task 0.0 in stage 0.0 (TID 0, localhost, executor driver): java.lang.ArrayIndexOutOfBoundsException: 7
	at org.apache.spark.streamdm.classifiers.trees.NominalFeatureClassObserver.observeClass(FeatureClassObserver.scala:155)
	at org.apache.spark.streamdm.classifiers.trees.nodes.ActiveLearningNode$$anonfun$learn$1.apply(ActiveLearningNode.scala:73)
	at org.apache.spark.streamdm.classifiers.trees.nodes.ActiveLearningNode$$anonfun$learn$1.apply(ActiveLearningNode.scala:73)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at org.apache.spark.streamdm.classifiers.trees.nodes.ActiveLearningNode.learn(ActiveLearningNode.scala:72)
	at org.apache.spark.streamdm.classifiers.trees.nodes.LearningNodeNBAdaptive.learn(LearningNodeNBAdaptive.scala:68)
	at org.apache.spark.streamdm.classifiers.trees.HoeffdingTreeModel.update(HoeffdingTree.scala:230)
	at org.apache.spark.streamdm.classifiers.trees.HoeffdingTree$$anonfun$train$1$$anonfun$1.apply(HoeffdingTree.scala:150)
	at org.apache.spark.streamdm.classifiers.trees.HoeffdingTree$$anonfun$train$1$$anonfun$1.apply(HoeffdingTree.scala:150)
	at scala.collection.TraversableOnce$$anonfun$foldLeft$1.apply(TraversableOnce.scala:157)
	at scala.collection.TraversableOnce$$anonfun$foldLeft$1.apply(TraversableOnce.scala:157)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)
	at scala.collection.TraversableOnce$class.foldLeft(TraversableOnce.scala:157)
	at org.apache.spark.InterruptibleIterator.foldLeft(InterruptibleIterator.scala:28)
	at scala.collection.TraversableOnce$class.aggregate(TraversableOnce.scala:214)
	at org.apache.spark.InterruptibleIterator.aggregate(InterruptibleIterator.scala:28)
	at org.apache.spark.rdd.RDD$$anonfun$aggregate$1$$anonfun$21.apply(RDD.scala:1122)
	at org.apache.spark.rdd.RDD$$anonfun$aggregate$1$$anonfun$21.apply(RDD.scala:1122)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:2130)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:2130)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:109)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

2019-10-29 15:11:12 INFO  TaskSchedulerImpl:54 - Removed TaskSet 0.0, whose tasks have all completed, from pool 
2019-10-29 15:11:12 INFO  TaskSchedulerImpl:54 - Cancelling stage 0
2019-10-29 15:11:12 INFO  DAGScheduler:54 - ResultStage 0 (aggregate at HoeffdingTree.scala:149) failed in 0,422 s due to Job aborted due to stage failure: Task 1 in stage 0.0 failed 1 times, most recent failure: Lost task 1.0 in stage 0.0 (TID 1, localhost, executor driver): java.lang.ArrayIndexOutOfBoundsException: 6
	at org.apache.spark.streamdm.classifiers.trees.NominalFeatureClassObserver.observeClass(FeatureClassObserver.scala:155)
	at org.apache.spark.streamdm.classifiers.trees.nodes.ActiveLearningNode$$anonfun$learn$1.apply(ActiveLearningNode.scala:73)
	at org.apache.spark.streamdm.classifiers.trees.nodes.ActiveLearningNode$$anonfun$learn$1.apply(ActiveLearningNode.scala:73)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at org.apache.spark.streamdm.classifiers.trees.nodes.ActiveLearningNode.learn(ActiveLearningNode.scala:72)
	at org.apache.spark.streamdm.classifiers.trees.nodes.LearningNodeNBAdaptive.learn(LearningNodeNBAdaptive.scala:68)
	at org.apache.spark.streamdm.classifiers.trees.HoeffdingTreeModel.update(HoeffdingTree.scala:230)
	at org.apache.spark.streamdm.classifiers.trees.HoeffdingTree$$anonfun$train$1$$anonfun$1.apply(HoeffdingTree.scala:150)
	at org.apache.spark.streamdm.classifiers.trees.HoeffdingTree$$anonfun$train$1$$anonfun$1.apply(HoeffdingTree.scala:150)
	at scala.collection.TraversableOnce$$anonfun$foldLeft$1.apply(TraversableOnce.scala:157)
	at scala.collection.TraversableOnce$$anonfun$foldLeft$1.apply(TraversableOnce.scala:157)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)
	at scala.collection.TraversableOnce$class.foldLeft(TraversableOnce.scala:157)
	at org.apache.spark.InterruptibleIterator.foldLeft(InterruptibleIterator.scala:28)
	at scala.collection.TraversableOnce$class.aggregate(TraversableOnce.scala:214)
	at org.apache.spark.InterruptibleIterator.aggregate(InterruptibleIterator.scala:28)
	at org.apache.spark.rdd.RDD$$anonfun$aggregate$1$$anonfun$21.apply(RDD.scala:1122)
	at org.apache.spark.rdd.RDD$$anonfun$aggregate$1$$anonfun$21.apply(RDD.scala:1122)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:2130)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:2130)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:109)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
2019-10-29 15:11:12 INFO  DAGScheduler:54 - Job 0 failed: aggregate at HoeffdingTree.scala:149, took 0,469394 s
2019-10-29 15:11:12 INFO  JobScheduler:54 - Finished job streaming job 1572372672000 ms.0 from job set of time 1572372672000 ms
2019-10-29 15:11:12 INFO  JobScheduler:54 - Starting job streaming job 1572372672000 ms.1 from job set of time 1572372672000 ms
2019-10-29 15:11:12 ERROR JobScheduler:91 - Error running job streaming job 1572372672000 ms.0
org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 0.0 failed 1 times, most recent failure: Lost task 1.0 in stage 0.0 (TID 1, localhost, executor driver): java.lang.ArrayIndexOutOfBoundsException: 6
	at org.apache.spark.streamdm.classifiers.trees.NominalFeatureClassObserver.observeClass(FeatureClassObserver.scala:155)
	at org.apache.spark.streamdm.classifiers.trees.nodes.ActiveLearningNode$$anonfun$learn$1.apply(ActiveLearningNode.scala:73)
	at org.apache.spark.streamdm.classifiers.trees.nodes.ActiveLearningNode$$anonfun$learn$1.apply(ActiveLearningNode.scala:73)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at org.apache.spark.streamdm.classifiers.trees.nodes.ActiveLearningNode.learn(ActiveLearningNode.scala:72)
	at org.apache.spark.streamdm.classifiers.trees.nodes.LearningNodeNBAdaptive.learn(LearningNodeNBAdaptive.scala:68)
	at org.apache.spark.streamdm.classifiers.trees.HoeffdingTreeModel.update(HoeffdingTree.scala:230)
	at org.apache.spark.streamdm.classifiers.trees.HoeffdingTree$$anonfun$train$1$$anonfun$1.apply(HoeffdingTree.scala:150)
	at org.apache.spark.streamdm.classifiers.trees.HoeffdingTree$$anonfun$train$1$$anonfun$1.apply(HoeffdingTree.scala:150)
	at scala.collection.TraversableOnce$$anonfun$foldLeft$1.apply(TraversableOnce.scala:157)
	at scala.collection.TraversableOnce$$anonfun$foldLeft$1.apply(TraversableOnce.scala:157)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)
	at scala.collection.TraversableOnce$class.foldLeft(TraversableOnce.scala:157)
	at org.apache.spark.InterruptibleIterator.foldLeft(InterruptibleIterator.scala:28)
	at scala.collection.TraversableOnce$class.aggregate(TraversableOnce.scala:214)
	at org.apache.spark.InterruptibleIterator.aggregate(InterruptibleIterator.scala:28)
	at org.apache.spark.rdd.RDD$$anonfun$aggregate$1$$anonfun$21.apply(RDD.scala:1122)
	at org.apache.spark.rdd.RDD$$anonfun$aggregate$1$$anonfun$21.apply(RDD.scala:1122)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:2130)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:2130)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:109)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1651)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1639)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1638)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1638)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)
	at scala.Option.foreach(Option.scala:257)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:831)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1872)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1821)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1810)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:642)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2034)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2131)
	at org.apache.spark.rdd.RDD$$anonfun$aggregate$1.apply(RDD.scala:1124)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:363)
	at org.apache.spark.rdd.RDD.aggregate(RDD.scala:1117)
	at org.apache.spark.streamdm.classifiers.trees.HoeffdingTree$$anonfun$train$1.apply(HoeffdingTree.scala:149)
	at org.apache.spark.streamdm.classifiers.trees.HoeffdingTree$$anonfun$train$1.apply(HoeffdingTree.scala:147)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$foreachRDD$1$$anonfun$apply$mcV$sp$3.apply(DStream.scala:628)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$foreachRDD$1$$anonfun$apply$mcV$sp$3.apply(DStream.scala:628)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ForEachDStream.scala:51)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:51)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:51)
	at org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:416)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply$mcV$sp(ForEachDStream.scala:50)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:50)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:50)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply$mcV$sp(JobScheduler.scala:257)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:257)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:257)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.ArrayIndexOutOfBoundsException: 6
	at org.apache.spark.streamdm.classifiers.trees.NominalFeatureClassObserver.observeClass(FeatureClassObserver.scala:155)
	at org.apache.spark.streamdm.classifiers.trees.nodes.ActiveLearningNode$$anonfun$learn$1.apply(ActiveLearningNode.scala:73)
	at org.apache.spark.streamdm.classifiers.trees.nodes.ActiveLearningNode$$anonfun$learn$1.apply(ActiveLearningNode.scala:73)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at org.apache.spark.streamdm.classifiers.trees.nodes.ActiveLearningNode.learn(ActiveLearningNode.scala:72)
	at org.apache.spark.streamdm.classifiers.trees.nodes.LearningNodeNBAdaptive.learn(LearningNodeNBAdaptive.scala:68)
	at org.apache.spark.streamdm.classifiers.trees.HoeffdingTreeModel.update(HoeffdingTree.scala:230)
	at org.apache.spark.streamdm.classifiers.trees.HoeffdingTree$$anonfun$train$1$$anonfun$1.apply(HoeffdingTree.scala:150)
	at org.apache.spark.streamdm.classifiers.trees.HoeffdingTree$$anonfun$train$1$$anonfun$1.apply(HoeffdingTree.scala:150)
	at scala.collection.TraversableOnce$$anonfun$foldLeft$1.apply(TraversableOnce.scala:157)
	at scala.collection.TraversableOnce$$anonfun$foldLeft$1.apply(TraversableOnce.scala:157)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)
	at scala.collection.TraversableOnce$class.foldLeft(TraversableOnce.scala:157)
	at org.apache.spark.InterruptibleIterator.foldLeft(InterruptibleIterator.scala:28)
	at scala.collection.TraversableOnce$class.aggregate(TraversableOnce.scala:214)
	at org.apache.spark.InterruptibleIterator.aggregate(InterruptibleIterator.scala:28)
	at org.apache.spark.rdd.RDD$$anonfun$aggregate$1$$anonfun$21.apply(RDD.scala:1122)
	at org.apache.spark.rdd.RDD$$anonfun$aggregate$1$$anonfun$21.apply(RDD.scala:1122)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:2130)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:2130)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:109)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)
	... 3 more
2019-10-29 15:11:12 INFO  StreamingContext:54 - Invoking stop(stopGracefully=false) from shutdown hook
2019-10-29 15:11:12 INFO  ReceiverTracker:54 - ReceiverTracker stopped
2019-10-29 15:11:12 INFO  JobGenerator:54 - Stopping JobGenerator immediately
2019-10-29 15:11:12 INFO  RecurringTimer:54 - Stopped timer for JobGenerator after time 1572372672000
2019-10-29 15:11:12 INFO  FileReader$$anon$1:54 - Reading file stopped.
2019-10-29 15:11:12 INFO  JobGenerator:54 - Stopped JobGenerator
2019-10-29 15:11:12 INFO  SparkContext:54 - Starting job: collect at PrintStreamWriter.scala:36
2019-10-29 15:11:12 INFO  DAGScheduler:54 - Registering RDD 3 (main at NativeMethodAccessorImpl.java:0)
2019-10-29 15:11:12 INFO  DAGScheduler:54 - Got job 1 (collect at PrintStreamWriter.scala:36) with 1 output partitions
2019-10-29 15:11:12 INFO  DAGScheduler:54 - Final stage: ResultStage 2 (collect at PrintStreamWriter.scala:36)
2019-10-29 15:11:12 INFO  DAGScheduler:54 - Parents of final stage: List(ShuffleMapStage 1)
2019-10-29 15:11:12 INFO  DAGScheduler:54 - Missing parents: List(ShuffleMapStage 1)
2019-10-29 15:11:12 INFO  DAGScheduler:54 - Submitting ShuffleMapStage 1 (MapPartitionsRDD[3] at main at NativeMethodAccessorImpl.java:0), which has no missing parents
2019-10-29 15:11:12 INFO  MemoryStore:54 - Block broadcast_1 stored as values in memory (estimated size 11.5 KB, free 366.3 MB)
2019-10-29 15:11:12 INFO  MemoryStore:54 - Block broadcast_1_piece0 stored as bytes in memory (estimated size 5.4 KB, free 366.3 MB)
2019-10-29 15:11:12 INFO  BlockManagerInfo:54 - Added broadcast_1_piece0 in memory on 192.168.15.12:40031 (size: 5.4 KB, free: 366.3 MB)
2019-10-29 15:11:12 INFO  SparkContext:54 - Created broadcast 1 from broadcast at DAGScheduler.scala:1039
2019-10-29 15:11:12 INFO  DAGScheduler:54 - Submitting 2 missing tasks from ShuffleMapStage 1 (MapPartitionsRDD[3] at main at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))
2019-10-29 15:11:12 INFO  TaskSchedulerImpl:54 - Adding task set 1.0 with 2 tasks
2019-10-29 15:11:12 INFO  TaskSetManager:54 - Starting task 0.0 in stage 1.0 (TID 2, localhost, executor driver, partition 0, PROCESS_LOCAL, 13284 bytes)
2019-10-29 15:11:12 INFO  TaskSetManager:54 - Starting task 1.0 in stage 1.0 (TID 3, localhost, executor driver, partition 1, PROCESS_LOCAL, 13284 bytes)
2019-10-29 15:11:12 INFO  Executor:54 - Running task 0.0 in stage 1.0 (TID 2)
2019-10-29 15:11:12 INFO  Executor:54 - Running task 1.0 in stage 1.0 (TID 3)
2019-10-29 15:11:12 INFO  Executor:54 - Finished task 0.0 in stage 1.0 (TID 2). 1068 bytes result sent to driver
2019-10-29 15:11:12 INFO  Executor:54 - Finished task 1.0 in stage 1.0 (TID 3). 1068 bytes result sent to driver
2019-10-29 15:11:12 INFO  TaskSetManager:54 - Finished task 1.0 in stage 1.0 (TID 3) in 111 ms on localhost (executor driver) (1/2)
2019-10-29 15:11:12 INFO  TaskSetManager:54 - Finished task 0.0 in stage 1.0 (TID 2) in 115 ms on localhost (executor driver) (2/2)
2019-10-29 15:11:12 INFO  TaskSchedulerImpl:54 - Removed TaskSet 1.0, whose tasks have all completed, from pool 
2019-10-29 15:11:12 INFO  DAGScheduler:54 - ShuffleMapStage 1 (main at NativeMethodAccessorImpl.java:0) finished in 0,131 s
2019-10-29 15:11:12 INFO  DAGScheduler:54 - looking for newly runnable stages
2019-10-29 15:11:12 INFO  DAGScheduler:54 - running: Set()
2019-10-29 15:11:12 INFO  DAGScheduler:54 - waiting: Set(ResultStage 2)
2019-10-29 15:11:12 INFO  DAGScheduler:54 - failed: Set()
2019-10-29 15:11:12 INFO  DAGScheduler:54 - Submitting ResultStage 2 (MapPartitionsRDD[6] at main at NativeMethodAccessorImpl.java:0), which has no missing parents
2019-10-29 15:11:12 INFO  MemoryStore:54 - Block broadcast_2 stored as values in memory (estimated size 6.8 KB, free 366.3 MB)
2019-10-29 15:11:12 INFO  MemoryStore:54 - Block broadcast_2_piece0 stored as bytes in memory (estimated size 3.7 KB, free 366.3 MB)
2019-10-29 15:11:12 INFO  BlockManagerInfo:54 - Added broadcast_2_piece0 in memory on 192.168.15.12:40031 (size: 3.7 KB, free: 366.3 MB)
2019-10-29 15:11:12 INFO  SparkContext:54 - Created broadcast 2 from broadcast at DAGScheduler.scala:1039
2019-10-29 15:11:12 INFO  DAGScheduler:54 - Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[6] at main at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
2019-10-29 15:11:12 INFO  TaskSchedulerImpl:54 - Adding task set 2.0 with 1 tasks
2019-10-29 15:11:12 INFO  TaskSetManager:54 - Starting task 0.0 in stage 2.0 (TID 4, localhost, executor driver, partition 0, ANY, 7649 bytes)
2019-10-29 15:11:12 INFO  Executor:54 - Running task 0.0 in stage 2.0 (TID 4)
2019-10-29 15:11:12 INFO  ShuffleBlockFetcherIterator:54 - Getting 2 non-empty blocks out of 2 blocks
2019-10-29 15:11:12 INFO  ShuffleBlockFetcherIterator:54 - Started 0 remote fetches in 6 ms
2019-10-29 15:11:12 INFO  Executor:54 - Finished task 0.0 in stage 2.0 (TID 4). 1184 bytes result sent to driver
2019-10-29 15:11:12 INFO  TaskSetManager:54 - Finished task 0.0 in stage 2.0 (TID 4) in 48 ms on localhost (executor driver) (1/1)
2019-10-29 15:11:12 INFO  TaskSchedulerImpl:54 - Removed TaskSet 2.0, whose tasks have all completed, from pool 
2019-10-29 15:11:12 INFO  DAGScheduler:54 - ResultStage 2 (collect at PrintStreamWriter.scala:36) finished in 0,063 s
2019-10-29 15:11:12 INFO  DAGScheduler:54 - Job 1 finished: collect at PrintStreamWriter.scala:36, took 0,222159 s
1,316,0,490,0,490,1,000,0,658,NaN,49,51,0,0
2019-10-29 15:11:12 INFO  JobScheduler:54 - Finished job streaming job 1572372672000 ms.1 from job set of time 1572372672000 ms
2019-10-29 15:11:12 INFO  JobScheduler:54 - Total delay: 0,897 s for time 1572372672000 ms (execution: 0,818 s)
2019-10-29 15:11:12 INFO  JobScheduler:54 - Stopped JobScheduler
2019-10-29 15:11:12 INFO  ContextHandler:910 - Stopped o.s.j.s.ServletContextHandler@37d871c2{/streaming,null,UNAVAILABLE,@Spark}
2019-10-29 15:11:12 INFO  ContextHandler:910 - Stopped o.s.j.s.ServletContextHandler@7a904f32{/streaming/batch,null,UNAVAILABLE,@Spark}
2019-10-29 15:11:12 INFO  ContextHandler:910 - Stopped o.s.j.s.ServletContextHandler@36fc05ff{/static/streaming,null,UNAVAILABLE,@Spark}
2019-10-29 15:11:12 INFO  StreamingContext:54 - StreamingContext stopped successfully
2019-10-29 15:11:12 INFO  SparkContext:54 - Invoking stop() from shutdown hook
2019-10-29 15:11:12 INFO  AbstractConnector:318 - Stopped Spark@72efb5c1{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2019-10-29 15:11:12 INFO  SparkUI:54 - Stopped Spark web UI at http://192.168.15.12:4040
2019-10-29 15:11:12 INFO  MapOutputTrackerMasterEndpoint:54 - MapOutputTrackerMasterEndpoint stopped!
2019-10-29 15:11:12 INFO  MemoryStore:54 - MemoryStore cleared
2019-10-29 15:11:12 INFO  BlockManager:54 - BlockManager stopped
2019-10-29 15:11:12 INFO  BlockManagerMaster:54 - BlockManagerMaster stopped
2019-10-29 15:11:12 INFO  OutputCommitCoordinator$OutputCommitCoordinatorEndpoint:54 - OutputCommitCoordinator stopped!
2019-10-29 15:11:12 INFO  SparkContext:54 - Successfully stopped SparkContext
2019-10-29 15:11:12 INFO  ShutdownHookManager:54 - Shutdown hook called
2019-10-29 15:11:12 INFO  ShutdownHookManager:54 - Deleting directory /tmp/spark-0b7e3a47-b4c1-453e-adac-a3b50b7716f1
2019-10-29 15:11:12 INFO  ShutdownHookManager:54 - Deleting directory /tmp/spark-aa79a927-c9bb-46e1-af5b-a8d81dde1b82
